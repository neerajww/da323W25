---
layout: page
title: Materials
permalink: /materials/
---

<!--- {% include image.html url="/_images/cover2.jpg" width=175 align="right" %} -->

## Why Multimodal?
- [*Multimodal interaction: A review*, M. Turk](https://www.sciencedirect.com/science/article/pii/S0167865513002584)
- [*Multimodal Data Fusion: An overview of methods, challenges, and prospects*, Lahat et al.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7214350)

## Uniomodal Representation Learning
- [*Efficient Estimation of Word Representations in Vector Space*, Mikolov et al.](https://arxiv.org/pdf/1301.3781)
- [*The Illustrated Word2vec*, Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)
- [*Intuition and Use-cases of embeddings in NLP and beyond*, Jay Alammar](https://www.youtube.com/watch?v=4-QoMdSqG_I)
- [*Very Deep Convolutional Networks for Large-Scale Image Recognition (VGGNet)*, Karen Simonyan, Andrew Zisserman](https://arxiv.org/pdf/1409.1556)
- [*CNN architectures for large-scale audio classification*, Hershey et al.](https://arxiv.org/pdf/1609.09430)
- 

## Self-supervised Representation Learning
- [*Deep Clustering for Unsupervised Learning of Visual Features*, Caron et al.](https://openaccess.thecvf.com/content_ECCV_2018/papers/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.pdf)
- [*A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)*, Chen et al.](https://arxiv.org/abs/2002.05709)
- [*A Survey on Self-Supervised Learning: Algorithms, Applications, and Future Trends*, Gui et al.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10559458)
- [*wav2vec2.0: A framework for self-supervised learning of speech representations*, Baevski et al.](https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf)

# Machine Learning
- [*Pattern Recognition and Machine Learning*, C. Bishop](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)  
- [*Distributed Representations of Words and Phrases and their Compositionality*, T. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean](https://arxiv.org/abs/1310.4546)  
- [*An Introduction to Variational Autoencoders*, D. P. Kingma, M. Welling](https://www.nowpublishers.com/article/Download/MAL-056)

## Multimodal Learning
- [*Look, Listen and Learn*, R. Arandjelović, A. Zisserman](https://openaccess.thecvf.com/content_ICCV_2017/papers/Arandjelovic_Look_Listen_and_ICCV_2017_paper.pdf)  
- [*Look, Listen and Learn More: Design choices for deep audio embeddings*, J. Cramer, H. Wu, J. Salamon, J. P. Bello](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf)  
- [*Learning Transferable Visual Models From Natural Language Supervision (CLIP)*, Alec Radford et al.](https://arxiv.org/abs/2103.00020)  
- [*Learning Audio-Language Representations (CLAP)*, Andonian et al.](https://arxiv.org/abs/2306.02596)  
- [*Speech2Face: Learning the Face Behind a Voice*, T. Wen et al.](https://arxiv.org/abs/1902.05766)  
- [*WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning*, Krishna Srinivasan et al.](https://arxiv.org/abs/2103.01913)

## Speech and Audio
- [*Deep Clustering and Conventional Networks for Music Separation: Strong Together*, J. R. Hershey et al.](https://arxiv.org/abs/1707.04668)

## Multimodal Learning: Newer domains
- [*Multimodal Biomedical AI*, Julián N. Acosta, Guido J. Falcone, Pranav Rajpurkar, Eric J. Topol](https://www.nature.com/articles/s41591-022-01981-2)
- [*BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity*, Zahra Gharaee, Scott C. Lowe, ZeMing Gong, Pablo Millan Arias, Nicholas Pellegrino, Austin T. Wang, Joakim Bruslund Haurum, Iuliia Zarubiieva, Lila Kari, Dirk Steinke, Graham W. Taylor, Paul Fieguth, Angel X. Chang](https://neurips.cc/media/neurips-2024/Slides/97824_NAvnRCG.pdf)


## Datasets
- [MINST]
- [FMINST]
- [CIFAR]
- [CANDOR]
- [Coswara]
- [ImageNet]
- [WIT]
- [Google Audio Dataset]
- [Vox celeb dataset]
